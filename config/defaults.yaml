defaults:
  # Job
  n_steps: 500_000
  run_name: none
  resume_id:
  offline_data_dir:
  offline_prefill_dir:
  offline_test_dir:
  offline_eval_dir:
  log_interval: 100
  logbatch_interval: 1000
  save_interval: 200
  eval_interval: 2000
  data_workers: 6  # need 6 to not bottleneck on unzipping npz, 4 is not enough
  enable_profiler: True
  verbose: False

  # Features
  image_key: image
  image_size: 64
  image_channels: 3
  image_categorical: False
  map_key:
  map_size: 11
  map_channels: 4
  map_categorical: True
  action_dim: 0
  
  # Training
  buffer_size: 10_000_000
  reset_interval: 200
  iwae_samples: 1
  imagine_dropout: 0
  kl_balance: 0.8
  kl_weight: 1.0
  image_weight: 1.0
  reward_weight: 1.0
  terminal_weight: 1.0
  adam_lr: 3.0e-4
  adam_lr_actor: 1.0e-4
  adam_lr_critic: 1.0e-4
  adam_eps: 1.0e-5
  keep_state: True
  batch_length: 50
  batch_size: 50
  device: "cuda:0"
  grad_clip: 1000
  grad_clip_ac: 200
  image_decoder_min_prob: 0
  amp: True

  # Eval
  test_batches: 61  # For unbiased test needs to be enough to cover full episodes
  test_batch_size: 10  # Smaller for faster test
  eval_batches: 61  # Big enough to reach episode end (xN). +1 to log last episode
  eval_samples: 10  # Big enough for good sampling
  eval_batch_size: 10  # Limited by GPU mem

  # Model
  model: dreamer
  embed_rnn: none
  deter_dim: 2048
  stoch_dim: 32
  stoch_discrete: 32
  hidden_dim: 1000
  gru_layers: 1
  gru_type: gru
  layer_norm: True
  image_encoder: cnn
  cnn_depth: 48
  image_encoder_layers: 0
  image_decoder: cnn
  image_decoder_layers: 0
  reward_input: True
  reward_decoder_layers: 4
  reward_decoder_categorical:
  terminal_decoder_layers: 4
  map_stoch_dim: 64
  map_model: none
  map_decoder: dense
  map_hidden_layers: 4
  map_hidden_dim: 1024
  mem_model: none
  mem_loss_type:
  global_dim: 0

  # Actor Critic
  gamma: 0.99
  lambda_gae: 0.95
  entropy: 0.003
  target_interval: 100
  imag_horizon: 15

  # Generator
  generator_workers: 4
  generator_prefill_steps: 500_000
  env_id:
  env_max_steps: 100_000  # not used
  env_no_terminal: False

minigrid:
  # Features
  image_key: image
  image_size: 7
  image_channels: 4
  image_categorical: True
  map_key: map
  map_size: 11
  map_channels: 4
  map_categorical: True
  action_dim: 7

  # Model
  image_encoder: dense
  image_encoder_layers: 3
  image_decoder: dense
  image_decoder_layers: 2
  map_model: direct

miniworld:
  # Features
  image_key: image
  image_size: 64
  image_channels: 3
  image_categorical: False
  map_key: map
  map_size: 9
  map_channels: 4
  map_categorical: True
  action_dim: 3

  # Model
  map_model: direct

atari:
  # Generator
  env_id: Atari-Pong
  action_dim: 18
  # env_max_steps: 27_000  # =108_000/action_repeat = 30 minutes of game play.

  # Model
  deter_dim: 1024

  # Optimizer
  kl_weight: 0.1

dmlab:
  env_id: DmLab-rooms_watermaze
  env_no_terminal: True
  action_dim: 9

minerl:
  env_id: MineRLObtainDiamondVectorObf-v0
  action_dim: 100
  generator_workers: 2
  entropy: 0.030

minerl_bc:
  env_id: MineRLObtainDiamondVectorObf-v0
  action_dim: 100
  model: bc
  batch_length: 8
  batch_size: 50